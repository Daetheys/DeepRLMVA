Class
    method1(arg1 : type, arg2 : type) -> out : type
    method2(arg1 : type) -> out : type



Trainer:
    __init__(agent : Agent,env_creator : () -> Environment,config : Dict)
    train(nb_steps : Int)
        Trains the agent for [nb_steps] in the environment (uses rollout to generate trajectories and then fits the agent on those trajectories using the ReplayBuffer)

ReplayBuffer:
    __init__()
    add(obs_tm1, action_tm1, reward_t, obs_t, discount_t, gae)
        Adds a timestep to the ReplayBuffer
    sample(batch_size : Int)->Array[?]
        Samples a batch of timesteps

Agent:
    __init__()
    select_action(observation : Array[Float]) -> action : Int
        Returns the action associated to the given observation according to the actual policy of the agent
    fit(batch : Array[?])
        Fit the PPO loss on the given batch

Environment:
    __init__()
    reset() -> observation : Array[Float]
        Resets the environment (to launch a new game)
    step(action : Int) -> observation, reward : Int, done : Bool, info : Dict[str,?]
        Steps in the environment (apply the given action and returns the new observation, the reward associated to this action, whether the game is ended and needs to be reset and additional information in a Dict)


Functions

rollout(agent : Agent, env : Environment, replay : ReplayBuffer, nb_steps : Int) -> stats : Dict[str,?]
    Runs the policy of the agent (via Agent.select_action and Environment.step) for [nb_steps], saves the timesteps in the ReplayBuffer (via ReplayBuffer.add - see with the ReplayBuffer under which form to put the data) and returns stats of the trajectories in outputs (reward mean, ...).

simple_net(out_dim : Int) -> init,apply (see TP 1 about haiku)
    Returns a small network using haiku.
